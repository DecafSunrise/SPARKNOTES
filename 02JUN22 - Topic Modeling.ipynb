{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "import pickle \n",
    "from pprint import pprint\n",
    "import re\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In resource or connectivity-constrained environments, consider saving these off as a text file or whatever\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e138f2",
   "metadata": {},
   "source": [
    "### Data\n",
    "- https://www.kaggle.com/datasets/fabiochiusano/medium-articles?resource=download\n",
    "### LDA\n",
    "- https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0  \n",
    "### Topic Coherence\n",
    "- https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0  \n",
    "### Tokenization help\n",
    "- https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b522f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\Work\\Data\\medium_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c6e2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4bbd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "      <td>https://medium.com/invisible-illness/mental-no...</td>\n",
       "      <td>['Ryan Fan']</td>\n",
       "      <td>2020-12-26 03:38:10.479000+00:00</td>\n",
       "      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n",
       "      <td>['Simon Spichak']</td>\n",
       "      <td>2020-09-23 22:10:17.126000+00:00</td>\n",
       "      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-10-10 20:17:37.132000+00:00</td>\n",
       "      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192363</th>\n",
       "      <td>Why do you need a cleaning service?</td>\n",
       "      <td>What could be more important than having a tid...</td>\n",
       "      <td>https://medium.com/@ozneedcleaningau/why-do-yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2021-11-16 08:17:08.950000+00:00</td>\n",
       "      <td>['Cleaning', 'Cleaning Services', 'Cleaning Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192364</th>\n",
       "      <td>Daily cleaning and maintenance of bedding</td>\n",
       "      <td>Daily cleaning and maintenance of bedding\\n\\nW...</td>\n",
       "      <td>https://medium.com/@a198blwt/daily-cleaning-an...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2021-11-16 05:27:05.359000+00:00</td>\n",
       "      <td>['Bedding', 'Cleaning', 'Maintain']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192365</th>\n",
       "      <td>Beneficial Advice on Bond Cleaning!</td>\n",
       "      <td>The most important chore at the end is bond cl...</td>\n",
       "      <td>https://medium.com/@princegohil/beneficial-adv...</td>\n",
       "      <td>['Prince Shrawan']</td>\n",
       "      <td>2021-11-26 08:20:27.660000+00:00</td>\n",
       "      <td>['Cleaning', 'End Of Lease Cleaning', 'Cleaners']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192366</th>\n",
       "      <td>How I Learned Romanian in 37 Easy Steps</td>\n",
       "      <td>How I Learned Romanian in 37 Easy Steps\\n\\nHey...</td>\n",
       "      <td>https://medium.com/@lifeinromania/how-i-learne...</td>\n",
       "      <td>['Sam Ursu']</td>\n",
       "      <td>2017-11-27 08:09:19.025000+00:00</td>\n",
       "      <td>['Romania', 'Language Learning', 'Storyofmylife']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192367</th>\n",
       "      <td>Trying Pimsleur Cantonese in Hong Kong</td>\n",
       "      <td>Over the past few years, I’ve heard a number o...</td>\n",
       "      <td>https://medium.com/toshuo/trying-pimsleur-cant...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2017-06-15 02:24:35.659000+00:00</td>\n",
       "      <td>['Hong Kong', 'Cantonese', 'Language Learning'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192368 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title  \\\n",
       "0                             Mental Note Vol. 24   \n",
       "1                       Your Brain On Coronavirus   \n",
       "2                                  Mind Your Nose   \n",
       "3                        The 4 Purposes of Dreams   \n",
       "4                Surviving a Rod Through the Head   \n",
       "...                                           ...   \n",
       "192363        Why do you need a cleaning service?   \n",
       "192364  Daily cleaning and maintenance of bedding   \n",
       "192365        Beneficial Advice on Bond Cleaning!   \n",
       "192366    How I Learned Romanian in 37 Easy Steps   \n",
       "192367     Trying Pimsleur Cantonese in Hong Kong   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n",
       "1       Your Brain On Coronavirus\\n\\nA guide to the cu...   \n",
       "2       Mind Your Nose\\n\\nHow smell training can chang...   \n",
       "3       Passionate about the synergy between science a...   \n",
       "4       You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "...                                                   ...   \n",
       "192363  What could be more important than having a tid...   \n",
       "192364  Daily cleaning and maintenance of bedding\\n\\nW...   \n",
       "192365  The most important chore at the end is bond cl...   \n",
       "192366  How I Learned Romanian in 37 Easy Steps\\n\\nHey...   \n",
       "192367  Over the past few years, I’ve heard a number o...   \n",
       "\n",
       "                                                      url  \\\n",
       "0       https://medium.com/invisible-illness/mental-no...   \n",
       "1       https://medium.com/age-of-awareness/how-the-pa...   \n",
       "2       https://medium.com/neodotlife/mind-your-nose-f...   \n",
       "3       https://medium.com/science-for-real/the-4-purp...   \n",
       "4       https://medium.com/live-your-life-on-purpose/s...   \n",
       "...                                                   ...   \n",
       "192363  https://medium.com/@ozneedcleaningau/why-do-yo...   \n",
       "192364  https://medium.com/@a198blwt/daily-cleaning-an...   \n",
       "192365  https://medium.com/@princegohil/beneficial-adv...   \n",
       "192366  https://medium.com/@lifeinromania/how-i-learne...   \n",
       "192367  https://medium.com/toshuo/trying-pimsleur-cant...   \n",
       "\n",
       "                       authors                         timestamp  \\\n",
       "0                 ['Ryan Fan']  2020-12-26 03:38:10.479000+00:00   \n",
       "1            ['Simon Spichak']  2020-09-23 22:10:17.126000+00:00   \n",
       "2                           []  2020-10-10 20:17:37.132000+00:00   \n",
       "3       ['Eshan Samaranayake']  2020-12-21 16:05:19.524000+00:00   \n",
       "4             ['Rishav Sinha']  2020-02-26 00:01:01.576000+00:00   \n",
       "...                        ...                               ...   \n",
       "192363                      []  2021-11-16 08:17:08.950000+00:00   \n",
       "192364                      []  2021-11-16 05:27:05.359000+00:00   \n",
       "192365      ['Prince Shrawan']  2021-11-26 08:20:27.660000+00:00   \n",
       "192366            ['Sam Ursu']  2017-11-27 08:09:19.025000+00:00   \n",
       "192367                      []  2017-06-15 02:24:35.659000+00:00   \n",
       "\n",
       "                                                     tags  \n",
       "0       ['Mental Health', 'Health', 'Psychology', 'Sci...  \n",
       "1       ['Mental Health', 'Coronavirus', 'Science', 'P...  \n",
       "2       ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n",
       "3       ['Health', 'Neuroscience', 'Mental Health', 'P...  \n",
       "4       ['Brain', 'Health', 'Development', 'Psychology...  \n",
       "...                                                   ...  \n",
       "192363  ['Cleaning', 'Cleaning Services', 'Cleaning Co...  \n",
       "192364                ['Bedding', 'Cleaning', 'Maintain']  \n",
       "192365  ['Cleaning', 'End Of Lease Cleaning', 'Cleaners']  \n",
       "192366  ['Romania', 'Language Learning', 'Storyofmylife']  \n",
       "192367  ['Hong Kong', 'Cantonese', 'Language Learning'...  \n",
       "\n",
       "[192368 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9ad85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "df['text_processed'] = df['text'].map(lambda x: re.sub('[,\\.!?]-_(){}', '', x))\n",
    "# Convert the titles to lowercase\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "df['text_processed'] = df['text_processed'].str.replace(\"\\n\", \" \")\n",
    "df['text_processed'] = df['text_processed'].str.replace(\"\\t\", \" \")\n",
    "df['text_processed'] = df['text_processed'].str.replace(pat=r\"\\s{2,}\", repl=\" \", regex=True)\n",
    "\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: \" \".join([word for word in x.split(' ') if word.isalnum()]))\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: \" \".join([word for word in x.split(' ') if len(word)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbd669f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        photo josh riemer unsplash merry christmas and...\n",
       "1        your brain coronavirus guide the curious and t...\n",
       "2        mind your nose how smell training can change y...\n",
       "3        passionate about the synergy between science a...\n",
       "4        heard phineas the railroad worker who survived...\n",
       "                               ...                        \n",
       "19995    the often portrayed with good and bad nerds sh...\n",
       "19996    the picture provided unsplash android notifica...\n",
       "19997    behalf geezers thank though think myself more ...\n",
       "19998    the age the new breed innovators lead the pres...\n",
       "19999    according the cost single data breach for comp...\n",
       "Name: text_processed, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ada340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "## Added this from the topic coherence article\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed5a703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['photo', 'josh', 'riemer', 'unsplash', 'merry', 'christmas', 'happy', 'wanted', 'everyone', 'know', 'much', 'appreciate', 'everyone', 'thankful', 'readers', 'writers', 'anywhere', 'without', 'thank', 'bringing', 'important', 'pieces', 'destigmatize', 'mental', 'illness', 'mental', 'without', 'ten', 'top', 'stories']\n"
     ]
    }
   ],
   "source": [
    "data = df.text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram model\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "# Faster way to get a sentence clubbed as a bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa15a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the trigram model\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d00c9345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAvis_data_filepath = os.getcwd()+\n",
    "\n",
    "LDAvis_data_filepath = os.path.join(f'{os.getcwd()}/results/ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'{os.getcwd()}/results/ldavis_prepared_'+ str(num_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01700263",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac85123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, f'ldavis_prepared_{str(num_topics)}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1330ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
