{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "807c6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "import pickle \n",
    "from pprint import pprint\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbeb7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2519b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Danny\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## In resource or connectivity-constrained environments, consider saving these off as a text file or whatever\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e138f2",
   "metadata": {},
   "source": [
    "### Data\n",
    "- https://www.kaggle.com/datasets/fabiochiusano/medium-articles?resource=download\n",
    "### LDA\n",
    "- https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0  \n",
    "### Topic Coherence\n",
    "- https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0  \n",
    "### Tokenization help\n",
    "- https://towardsdatascience.com/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b522f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\Work\\Data\\medium_articles.csv\", nrows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4bbd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "      <td>https://medium.com/invisible-illness/mental-no...</td>\n",
       "      <td>['Ryan Fan']</td>\n",
       "      <td>2020-12-26 03:38:10.479000+00:00</td>\n",
       "      <td>['Mental Health', 'Health', 'Psychology', 'Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "      <td>https://medium.com/age-of-awareness/how-the-pa...</td>\n",
       "      <td>['Simon Spichak']</td>\n",
       "      <td>2020-09-23 22:10:17.126000+00:00</td>\n",
       "      <td>['Mental Health', 'Coronavirus', 'Science', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "      <td>https://medium.com/neodotlife/mind-your-nose-f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-10-10 20:17:37.132000+00:00</td>\n",
       "      <td>['Biotechnology', 'Neuroscience', 'Brain', 'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "      <td>https://medium.com/science-for-real/the-4-purp...</td>\n",
       "      <td>['Eshan Samaranayake']</td>\n",
       "      <td>2020-12-21 16:05:19.524000+00:00</td>\n",
       "      <td>['Health', 'Neuroscience', 'Mental Health', 'P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "      <td>https://medium.com/live-your-life-on-purpose/s...</td>\n",
       "      <td>['Rishav Sinha']</td>\n",
       "      <td>2020-02-26 00:01:01.576000+00:00</td>\n",
       "      <td>['Brain', 'Health', 'Development', 'Psychology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>NLP visualizations for clear, immediate insigh...</td>\n",
       "      <td>NLP visualizations for clear, immediate insigh...</td>\n",
       "      <td>https://medium.com/plotly/nlp-visualisations-f...</td>\n",
       "      <td>['Jp Hwang']</td>\n",
       "      <td>2020-03-30 16:37:04.161000+00:00</td>\n",
       "      <td>['Data Science', 'Programming', 'Python', 'Mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Swift — Creating a Custom View From a XIB (Upd...</td>\n",
       "      <td>9. Use Your View\\n\\nWe’re done! Now we’ve got ...</td>\n",
       "      <td>https://medium.com/better-programming/swift-3-...</td>\n",
       "      <td>['Brian Clouser']</td>\n",
       "      <td>2019-07-02 23:03:29.913000+00:00</td>\n",
       "      <td>['iOS', 'Swift', 'Mobile App Development', 'Sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Can You Solve The Mystery Of Edgar Allan Poe’s...</td>\n",
       "      <td>Can You Solve The Mystery Of Edgar Allan Poe’s...</td>\n",
       "      <td>https://medium.com/omgfacts/can-you-solve-the-...</td>\n",
       "      <td>['New Visions']</td>\n",
       "      <td>2016-09-23 22:05:12.751000+00:00</td>\n",
       "      <td>['Edgar Allan Poe', 'Mind', 'Poetry', 'Crime',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Your Life Is Full of Porn. Stop Getting Yourse...</td>\n",
       "      <td>This Is Your Life On Porn\\n\\nYou wake up. You ...</td>\n",
       "      <td>https://medium.com/the-ascent/your-life-is-ful...</td>\n",
       "      <td>['Tim Denning']</td>\n",
       "      <td>2020-07-31 17:01:01.525000+00:00</td>\n",
       "      <td>['Addiction', 'Life', 'Money', 'Social Media',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Truly Customizing Power BI with React, Angular...</td>\n",
       "      <td>With the growth of the amount of data availabl...</td>\n",
       "      <td>https://towardsdatascience.com/truly-customizi...</td>\n",
       "      <td>['Thiago Candido']</td>\n",
       "      <td>2020-09-14 13:28:41.472000+00:00</td>\n",
       "      <td>['React', 'Angular', 'Power Bi', 'Web Developm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                   Mental Note Vol. 24   \n",
       "1                             Your Brain On Coronavirus   \n",
       "2                                        Mind Your Nose   \n",
       "3                              The 4 Purposes of Dreams   \n",
       "4                      Surviving a Rod Through the Head   \n",
       "...                                                 ...   \n",
       "4995  NLP visualizations for clear, immediate insigh...   \n",
       "4996  Swift — Creating a Custom View From a XIB (Upd...   \n",
       "4997  Can You Solve The Mystery Of Edgar Allan Poe’s...   \n",
       "4998  Your Life Is Full of Porn. Stop Getting Yourse...   \n",
       "4999  Truly Customizing Power BI with React, Angular...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...   \n",
       "1     Your Brain On Coronavirus\\n\\nA guide to the cu...   \n",
       "2     Mind Your Nose\\n\\nHow smell training can chang...   \n",
       "3     Passionate about the synergy between science a...   \n",
       "4     You’ve heard of him, haven’t you? Phineas Gage...   \n",
       "...                                                 ...   \n",
       "4995  NLP visualizations for clear, immediate insigh...   \n",
       "4996  9. Use Your View\\n\\nWe’re done! Now we’ve got ...   \n",
       "4997  Can You Solve The Mystery Of Edgar Allan Poe’s...   \n",
       "4998  This Is Your Life On Porn\\n\\nYou wake up. You ...   \n",
       "4999  With the growth of the amount of data availabl...   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://medium.com/invisible-illness/mental-no...   \n",
       "1     https://medium.com/age-of-awareness/how-the-pa...   \n",
       "2     https://medium.com/neodotlife/mind-your-nose-f...   \n",
       "3     https://medium.com/science-for-real/the-4-purp...   \n",
       "4     https://medium.com/live-your-life-on-purpose/s...   \n",
       "...                                                 ...   \n",
       "4995  https://medium.com/plotly/nlp-visualisations-f...   \n",
       "4996  https://medium.com/better-programming/swift-3-...   \n",
       "4997  https://medium.com/omgfacts/can-you-solve-the-...   \n",
       "4998  https://medium.com/the-ascent/your-life-is-ful...   \n",
       "4999  https://towardsdatascience.com/truly-customizi...   \n",
       "\n",
       "                     authors                         timestamp  \\\n",
       "0               ['Ryan Fan']  2020-12-26 03:38:10.479000+00:00   \n",
       "1          ['Simon Spichak']  2020-09-23 22:10:17.126000+00:00   \n",
       "2                         []  2020-10-10 20:17:37.132000+00:00   \n",
       "3     ['Eshan Samaranayake']  2020-12-21 16:05:19.524000+00:00   \n",
       "4           ['Rishav Sinha']  2020-02-26 00:01:01.576000+00:00   \n",
       "...                      ...                               ...   \n",
       "4995            ['Jp Hwang']  2020-03-30 16:37:04.161000+00:00   \n",
       "4996       ['Brian Clouser']  2019-07-02 23:03:29.913000+00:00   \n",
       "4997         ['New Visions']  2016-09-23 22:05:12.751000+00:00   \n",
       "4998         ['Tim Denning']  2020-07-31 17:01:01.525000+00:00   \n",
       "4999      ['Thiago Candido']  2020-09-14 13:28:41.472000+00:00   \n",
       "\n",
       "                                                   tags  \n",
       "0     ['Mental Health', 'Health', 'Psychology', 'Sci...  \n",
       "1     ['Mental Health', 'Coronavirus', 'Science', 'P...  \n",
       "2     ['Biotechnology', 'Neuroscience', 'Brain', 'We...  \n",
       "3     ['Health', 'Neuroscience', 'Mental Health', 'P...  \n",
       "4     ['Brain', 'Health', 'Development', 'Psychology...  \n",
       "...                                                 ...  \n",
       "4995  ['Data Science', 'Programming', 'Python', 'Mac...  \n",
       "4996  ['iOS', 'Swift', 'Mobile App Development', 'Sw...  \n",
       "4997  ['Edgar Allan Poe', 'Mind', 'Poetry', 'Crime',...  \n",
       "4998  ['Addiction', 'Life', 'Money', 'Social Media',...  \n",
       "4999  ['React', 'Angular', 'Power Bi', 'Web Developm...  \n",
       "\n",
       "[5000 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ad85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "df['text_processed'] = df['text'].map(lambda x: re.sub('[,\\.!?]-_(){}', '', x))\n",
    "# Convert the titles to lowercase\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "df['text_processed'] = df['text_processed'].str.replace(\"\\n\", \" \")\n",
    "df['text_processed'] = df['text_processed'].str.replace(\"\\t\", \" \")\n",
    "df['text_processed'] = df['text_processed'].str.replace(pat=r\"\\s{2,}\", repl=\" \", regex=True)\n",
    "\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: \" \".join([word for word in x.split(' ') if word.isalnum()]))\n",
    "df['text_processed'] = df['text_processed'].map(lambda x: \" \".join([word for word in x.split(' ') if len(word)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd669f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       photo josh riemer unsplash merry christmas and...\n",
       "1       your brain coronavirus guide the curious and t...\n",
       "2       mind your nose how smell training can change y...\n",
       "3       passionate about the synergy between science a...\n",
       "4       heard phineas the railroad worker who survived...\n",
       "                              ...                        \n",
       "4995    nlp visualizations for immediate insights into...\n",
       "4996    use your view now got custom made from xib tha...\n",
       "4997    can you solve the mystery edgar allan the evid...\n",
       "4998    this your life porn you wake you the toilet an...\n",
       "4999    with the growth the amount data available pres...\n",
       "Name: text_processed, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ada340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "## Added this from the topic coherence article\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed5a703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['photo', 'josh', 'riemer', 'unsplash', 'merry', 'christmas', 'happy', 'wanted', 'everyone', 'know', 'much', 'appreciate', 'everyone', 'thankful', 'readers', 'writers', 'anywhere', 'without', 'thank', 'bringing', 'important', 'pieces', 'destigmatize', 'mental', 'illness', 'mental', 'without', 'ten', 'top', 'stories']\n"
     ]
    }
   ],
   "source": [
    "data = df.text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# Remove Stop Words\n",
    "# data_words = remove_stopwords(data_words)\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3bab0",
   "metadata": {},
   "source": [
    "## Optional: Make Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a75e3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram model\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "# Faster way to get a sentence clubbed as a bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa15a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the trigram model\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "# Faster way to get a sentence clubbed as a trigram\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5f8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeef8562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['happy', 'want', 'know', 'much', 'appreciate', 'thankful', 'reader', 'writer', 'anywhere', 'thank', 'bring', 'important', 'piece', 'destigmatize', 'mental_illness', 'mental', 'top', 'story', 'last', 'capacity', 'love', 'inspire', 'universal', 'capacity', 'hate', 'irrespective', 'age', 'religion', 'none', 'exempt', 'aggressive', 'accordingly', 'repress', 'deep', 'seat', 'feeling', 'inferiority', 'inflate', 'delusion', 'grandeur', 'prone', 'aggression', 'infiltrate', 'interaction', 'myriad', 'environment', 'school', 'cyber', 'bullying', 'happen', 'ringleader', 'look', 'minion', 'sanction', 'cruelty', 'look', 'circumstance', 'bring', 'sad', 'grateful', 'program', 'change', 'life', 'help', 'imagine', 'life', 'like', 'learn', 'accept', 'powerlessness', 'prioritize', 'take', 'life', 'step', 'never', 'bet', 'world', 'much', 'prospect', 'spending', 'horrible', 'locked', 'psychiatric', 'low', 'point', 'day', 'room', 'festoon', 'cheesy', 'decoration', 'sorry', 'pink', 'aluminum', 'therapy', 'revolved_around', 'bake', 'decorate', 'fashioned', 'clay', 'ornament', 'turn', 'heavy', 'crappy', 'carol', 'background', 'hard', 'get', 'pissed', 'staff', 'make', 'good', 'hate', 'admit', 'even', 'never', 'betray', 'still', 'set', 'impossible', 'job', 'define', 'make', 'find', 'peace', 'contentment', 'else', 'personal', 'significant', 'feeling', 'loss', 'sadness', 'still', 'flare', 'time', 'reason', 'matter', 'resilient', 'purport', 'emotionally', 'vulnerable', 'human', 'talk', 'conceptual', 'loss', 'mechanically', 'away', 'talk', 'loss', 'sister', 'week', 'hard', 'case', 'continue', 'explode', 'government', 'leadership', 'remain', 'control', 'thing', 'take', 'deep', 'remain', 'vigilant', 'come', 'limit', 'exposure', 'let', 'lot', 'stuff', 'always', 'recognize', 'hopeful', 'able', 'tame', 'anecdotal', 'news', 'report', 'informal', 'evidence', 'isolation', 'boon', 'rather', 'study', 'mixed', 'emotion', 'show', 'low', 'emotional', 'stability', 'actually', 'well', 'respond', 'day', 'wish', 'heart', 'soul', 'result', 'virus', 'percent', 'people', 'experience', 'serious', 'infection', 'develop', 'visualize', 'life', 'cfs', 'year', 'smell', 'life', 'taste', 'smell', 'lavender', 'field', 'long', 'run', 'taste', 'meal', 'favorite', 'restaurant', 'long', 'walk', 'tip', 'thing', 'cross', 'chasm', 'potential', 'cabin', 'general', 'covid', 'year', 'cause', 'exposed', 'uncertainty', 'uncertainty', 'create', 'unease', 'feeling', 'take', 'year', 'motivate', 'plan', 'dream', 'prepare', 'inspire', 'future', 'cause', 'become', 'task', 'tricky', 'always', 'want', 'push', 'realise', 'lead', 'damage', 'important', 'notice', 'time', 'harsh', 'easily', 'turn', 'link', 'well', 'quality']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d00c9345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3b4f5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"business\" + 0.010*\"company\" + 0.010*\"get\" + 0.009*\"make\" + '\n",
      "  '0.008*\"marketing\" + 0.007*\"people\" + 0.007*\"content\" + 0.006*\"new\" + '\n",
      "  '0.006*\"email\" + 0.006*\"create\"'),\n",
      " (1,\n",
      "  '0.011*\"design\" + 0.009*\"make\" + 0.008*\"user\" + 0.008*\"product\" + '\n",
      "  '0.008*\"work\" + 0.007*\"team\" + 0.007*\"new\" + 0.007*\"app\" + 0.006*\"need\" + '\n",
      "  '0.006*\"use\"'),\n",
      " (2,\n",
      "  '0.015*\"get\" + 0.011*\"make\" + 0.011*\"time\" + 0.010*\"work\" + 0.009*\"take\" + '\n",
      "  '0.009*\"know\" + 0.008*\"people\" + 0.008*\"thing\" + 0.007*\"go\" + 0.007*\"feel\"'),\n",
      " (3,\n",
      "  '0.018*\"use\" + 0.010*\"create\" + 0.009*\"code\" + 0.009*\"image\" + 0.009*\"model\" '\n",
      "  '+ 0.008*\"function\" + 0.008*\"datum\" + 0.006*\"value\" + 0.006*\"make\" + '\n",
      "  '0.006*\"add\"'),\n",
      " (4,\n",
      "  '0.009*\"people\" + 0.007*\"study\" + 0.007*\"also\" + 0.006*\"health\" + '\n",
      "  '0.005*\"case\" + 0.005*\"get\" + 0.005*\"virus\" + 0.004*\"say\" + 0.004*\"many\" + '\n",
      "  '0.004*\"make\"'),\n",
      " (5,\n",
      "  '0.010*\"human\" + 0.008*\"datum\" + 0.006*\"make\" + 0.006*\"use\" + 0.005*\"system\" '\n",
      "  '+ 0.005*\"also\" + 0.004*\"brain\" + 0.004*\"technology\" + 0.004*\"machine\" + '\n",
      "  '0.004*\"many\"'),\n",
      " (6,\n",
      "  '0.015*\"service\" + 0.013*\"use\" + 0.013*\"run\" + 0.009*\"application\" + '\n",
      "  '0.009*\"create\" + 0.007*\"need\" + 0.007*\"cloud\" + 0.006*\"kubernete\" + '\n",
      "  '0.006*\"datum\" + 0.006*\"make\"'),\n",
      " (7,\n",
      "  '0.061*\"datum\" + 0.011*\"use\" + 0.007*\"model\" + 0.007*\"value\" + 0.007*\"time\" '\n",
      "  '+ 0.007*\"data\" + 0.006*\"feature\" + 0.006*\"dataset\" + 0.006*\"cluster\" + '\n",
      "  '0.006*\"number\"'),\n",
      " (8,\n",
      "  '0.016*\"write\" + 0.010*\"make\" + 0.010*\"story\" + 0.008*\"people\" + 0.008*\"get\" '\n",
      "  '+ 0.008*\"book\" + 0.007*\"read\" + 0.007*\"want\" + 0.006*\"feel\" + '\n",
      "  '0.006*\"writer\"'),\n",
      " (9,\n",
      "  '0.008*\"people\" + 0.007*\"time\" + 0.007*\"help\" + 0.007*\"get\" + 0.006*\"make\" + '\n",
      "  '0.005*\"take\" + 0.005*\"also\" + 0.004*\"food\" + 0.004*\"say\" + 0.004*\"change\"')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7164e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.39647229659125294\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05d8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.011*\"business\" + 0.009*\"get\" + 0.009*\"make\" + 0.009*\"company\" + '\n",
      "  '0.008*\"marketing\" + 0.007*\"people\" + 0.007*\"content\" + 0.006*\"new\" + '\n",
      "  '0.006*\"email\" + 0.006*\"create\"'),\n",
      " (1,\n",
      "  '0.010*\"design\" + 0.009*\"make\" + 0.009*\"user\" + 0.008*\"product\" + '\n",
      "  '0.008*\"work\" + 0.008*\"team\" + 0.007*\"new\" + 0.007*\"need\" + 0.007*\"app\" + '\n",
      "  '0.006*\"help\"'),\n",
      " (2,\n",
      "  '0.015*\"get\" + 0.011*\"make\" + 0.011*\"time\" + 0.010*\"work\" + 0.009*\"take\" + '\n",
      "  '0.009*\"know\" + 0.008*\"people\" + 0.008*\"thing\" + 0.007*\"go\" + 0.007*\"feel\"'),\n",
      " (3,\n",
      "  '0.018*\"use\" + 0.011*\"create\" + 0.009*\"datum\" + 0.009*\"model\" + 0.008*\"code\" '\n",
      "  '+ 0.007*\"image\" + 0.007*\"function\" + 0.006*\"make\" + 0.006*\"need\" + '\n",
      "  '0.006*\"set\"'),\n",
      " (4,\n",
      "  '0.009*\"people\" + 0.007*\"study\" + 0.006*\"also\" + 0.006*\"health\" + '\n",
      "  '0.005*\"get\" + 0.005*\"virus\" + 0.004*\"case\" + 0.004*\"many\" + 0.004*\"make\" + '\n",
      "  '0.004*\"say\"'),\n",
      " (5,\n",
      "  '0.010*\"human\" + 0.009*\"idea\" + 0.008*\"datum\" + 0.006*\"make\" + 0.005*\"use\" + '\n",
      "  '0.005*\"brain\" + 0.005*\"also\" + 0.005*\"people\" + 0.005*\"many\" + '\n",
      "  '0.005*\"even\"'),\n",
      " (6,\n",
      "  '0.011*\"run\" + 0.011*\"kubernete\" + 0.011*\"use\" + 0.009*\"service\" + '\n",
      "  '0.007*\"cloud\" + 0.007*\"application\" + 0.006*\"hadoop\" + 0.006*\"make\" + '\n",
      "  '0.006*\"need\" + 0.006*\"system\"'),\n",
      " (7,\n",
      "  '0.057*\"datum\" + 0.011*\"use\" + 0.008*\"time\" + 0.006*\"data\" + 0.006*\"value\" + '\n",
      "  '0.005*\"service\" + 0.005*\"get\" + 0.005*\"cluster\" + 0.005*\"feature\" + '\n",
      "  '0.005*\"model\"'),\n",
      " (8,\n",
      "  '0.016*\"write\" + 0.010*\"story\" + 0.010*\"make\" + 0.008*\"read\" + 0.008*\"book\" '\n",
      "  '+ 0.008*\"people\" + 0.007*\"get\" + 0.006*\"want\" + 0.006*\"feel\" + '\n",
      "  '0.006*\"writer\"'),\n",
      " (9,\n",
      "  '0.008*\"people\" + 0.007*\"get\" + 0.007*\"time\" + 0.007*\"help\" + 0.006*\"make\" + '\n",
      "  '0.005*\"take\" + 0.005*\"also\" + 0.005*\"say\" + 0.004*\"change\" + 0.004*\"use\"')]\n",
      "\n",
      "Coherence Score:  0.3818114433666241\n",
      "\n",
      "\n",
      " ------------ \n",
      "\n",
      "\n",
      "[(0,\n",
      "  '0.016*\"business\" + 0.011*\"company\" + 0.011*\"marketing\" + 0.010*\"get\" + '\n",
      "  '0.009*\"product\" + 0.009*\"make\" + 0.009*\"customer\" + 0.007*\"create\" + '\n",
      "  '0.007*\"brand\" + 0.007*\"people\"'),\n",
      " (1,\n",
      "  '0.014*\"design\" + 0.009*\"user\" + 0.009*\"make\" + 0.009*\"team\" + '\n",
      "  '0.009*\"product\" + 0.008*\"work\" + 0.008*\"new\" + 0.007*\"app\" + 0.007*\"need\" + '\n",
      "  '0.006*\"help\"'),\n",
      " (2,\n",
      "  '0.009*\"make\" + 0.008*\"get\" + 0.008*\"take\" + 0.007*\"people\" + 0.007*\"work\" + '\n",
      "  '0.007*\"time\" + 0.006*\"even\" + 0.006*\"know\" + 0.005*\"see\" + 0.005*\"come\"'),\n",
      " (3,\n",
      "  '0.019*\"use\" + 0.012*\"create\" + 0.010*\"image\" + 0.009*\"code\" + 0.009*\"model\" '\n",
      "  '+ 0.009*\"function\" + 0.008*\"datum\" + 0.007*\"file\" + 0.007*\"value\" + '\n",
      "  '0.007*\"add\"'),\n",
      " (4,\n",
      "  '0.008*\"get\" + 0.006*\"test\" + 0.006*\"make\" + 0.006*\"exam\" + 0.006*\"body\" + '\n",
      "  '0.005*\"exercise\" + 0.005*\"weight\" + 0.005*\"commit\" + 0.005*\"certification\" '\n",
      "  '+ 0.004*\"help\"'),\n",
      " (5,\n",
      "  '0.030*\"datum\" + 0.009*\"use\" + 0.009*\"human\" + 0.008*\"make\" + '\n",
      "  '0.006*\"machine\" + 0.006*\"science\" + 0.005*\"also\" + 0.005*\"machine_learne\" + '\n",
      "  '0.005*\"information\" + 0.005*\"many\"'),\n",
      " (6,\n",
      "  '0.014*\"application\" + 0.014*\"use\" + 0.013*\"run\" + 0.009*\"code\" + '\n",
      "  '0.008*\"also\" + 0.007*\"cloud\" + 0.007*\"service\" + 0.007*\"make\" + '\n",
      "  '0.007*\"need\" + 0.006*\"web\"'),\n",
      " (7,\n",
      "  '0.041*\"datum\" + 0.012*\"use\" + 0.009*\"service\" + 0.007*\"create\" + '\n",
      "  '0.007*\"cluster\" + 0.007*\"time\" + 0.006*\"feature\" + 0.005*\"value\" + '\n",
      "  '0.005*\"dataset\" + 0.005*\"need\"'),\n",
      " (8,\n",
      "  '0.019*\"story\" + 0.009*\"make\" + 0.007*\"write\" + 0.007*\"writer\" + 0.006*\"way\" '\n",
      "  '+ 0.006*\"people\" + 0.005*\"see\" + 0.005*\"read\" + 0.005*\"also\" + '\n",
      "  '0.005*\"find\"'),\n",
      " (9,\n",
      "  '0.009*\"time\" + 0.007*\"make\" + 0.007*\"get\" + 0.007*\"help\" + 0.006*\"people\" + '\n",
      "  '0.006*\"also\" + 0.006*\"work\" + 0.005*\"interview\" + 0.005*\"take\" + '\n",
      "  '0.005*\"change\"'),\n",
      " (10,\n",
      "  '0.009*\"people\" + 0.007*\"also\" + 0.007*\"health\" + 0.006*\"case\" + '\n",
      "  '0.006*\"virus\" + 0.005*\"study\" + 0.005*\"new\" + 0.005*\"say\" + 0.005*\"many\" + '\n",
      "  '0.005*\"risk\"'),\n",
      " (11,\n",
      "  '0.015*\"get\" + 0.014*\"write\" + 0.013*\"book\" + 0.012*\"make\" + 0.012*\"read\" + '\n",
      "  '0.011*\"work\" + 0.010*\"people\" + 0.009*\"article\" + 0.009*\"want\" + '\n",
      "  '0.008*\"post\"'),\n",
      " (12,\n",
      "  '0.013*\"brain\" + 0.011*\"people\" + 0.008*\"experience\" + 0.008*\"human\" + '\n",
      "  '0.007*\"social\" + 0.006*\"make\" + 0.005*\"way\" + 0.005*\"anxiety\" + '\n",
      "  '0.005*\"also\" + 0.005*\"get\"'),\n",
      " (13,\n",
      "  '0.033*\"energy\" + 0.007*\"power\" + 0.007*\"sound\" + 0.006*\"cost\" + 0.006*\"air\" '\n",
      "  '+ 0.006*\"use\" + 0.006*\"technology\" + 0.005*\"electricity\" + '\n",
      "  '0.005*\"renewable\" + 0.005*\"solar\"'),\n",
      " (14,\n",
      "  '0.019*\"headline\" + 0.015*\"type\" + 0.012*\"cell\" + 0.009*\"human\" + '\n",
      "  '0.007*\"lock\" + 0.006*\"study\" + 0.006*\"many\" + 0.006*\"thread\" + '\n",
      "  '0.005*\"brain\" + 0.005*\"neuron\"'),\n",
      " (15,\n",
      "  '0.017*\"get\" + 0.014*\"feel\" + 0.013*\"make\" + 0.012*\"time\" + 0.011*\"know\" + '\n",
      "  '0.010*\"thing\" + 0.010*\"people\" + 0.009*\"work\" + 0.009*\"take\" + '\n",
      "  '0.009*\"think\"'),\n",
      " (16,\n",
      "  '0.010*\"symptom\" + 0.009*\"card\" + 0.008*\"suicide\" + 0.008*\"study\" + '\n",
      "  '0.006*\"also\" + 0.005*\"tree\" + 0.005*\"use\" + 0.004*\"predict\" + '\n",
      "  '0.004*\"admission\" + 0.004*\"singer\"'),\n",
      " (17,\n",
      "  '0.011*\"people\" + 0.008*\"make\" + 0.006*\"feel\" + 0.006*\"know\" + 0.006*\"find\" '\n",
      "  '+ 0.005*\"think\" + 0.005*\"even\" + 0.005*\"eat\" + 0.005*\"get\" + 0.005*\"way\"'),\n",
      " (18,\n",
      "  '0.043*\"write\" + 0.018*\"get\" + 0.013*\"time\" + 0.013*\"work\" + 0.012*\"make\" + '\n",
      "  '0.012*\"want\" + 0.011*\"writing\" + 0.010*\"idea\" + 0.010*\"take\" + 0.010*\"day\"'),\n",
      " (19,\n",
      "  '0.009*\"water\" + 0.008*\"plant\" + 0.007*\"food\" + 0.006*\"quantum\" + '\n",
      "  '0.006*\"song\" + 0.004*\"make\" + 0.004*\"also\" + 0.004*\"ocean\" + 0.004*\"use\" + '\n",
      "  '0.004*\"skaffold\"')]\n",
      "\n",
      "Coherence Score:  0.3889967631588316\n",
      "\n",
      "\n",
      " ------------ \n",
      "\n",
      "\n",
      "[(25,\n",
      "  '0.045*\"app\" + 0.016*\"tool\" + 0.015*\"user\" + 0.011*\"development\" + '\n",
      "  '0.011*\"help\" + 0.011*\"make\" + 0.010*\"feature\" + 0.010*\"mobile\" + '\n",
      "  '0.008*\"use\" + 0.007*\"test\"'),\n",
      " (3,\n",
      "  '0.019*\"use\" + 0.014*\"image\" + 0.012*\"function\" + 0.010*\"value\" + '\n",
      "  '0.009*\"model\" + 0.009*\"create\" + 0.008*\"import\" + 0.008*\"code\" + '\n",
      "  '0.007*\"object\" + 0.007*\"datum\"'),\n",
      " (10,\n",
      "  '0.008*\"company\" + 0.008*\"also\" + 0.007*\"startup\" + 0.007*\"product\" + '\n",
      "  '0.006*\"team\" + 0.005*\"help\" + 0.005*\"vitamin\" + 0.005*\"new\" + '\n",
      "  '0.005*\"increase\" + 0.004*\"make\"'),\n",
      " (20,\n",
      "  '0.015*\"water\" + 0.011*\"plastic\" + 0.010*\"climate\" + 0.008*\"also\" + '\n",
      "  '0.007*\"carbon\" + 0.007*\"plant\" + 0.007*\"temperature\" + '\n",
      "  '0.007*\"environmental\" + 0.006*\"find\" + 0.006*\"ocean\"'),\n",
      " (4,\n",
      "  '0.012*\"exam\" + 0.010*\"certification\" + 0.007*\"exercise\" + 0.007*\"door\" + '\n",
      "  '0.006*\"vagus_nerve\" + 0.006*\"get\" + 0.005*\"practice\" + 0.005*\"want\" + '\n",
      "  '0.005*\"see\" + 0.005*\"muscle\"'),\n",
      " (0,\n",
      "  '0.041*\"business\" + 0.017*\"company\" + 0.015*\"marketing\" + 0.013*\"product\" + '\n",
      "  '0.010*\"get\" + 0.008*\"people\" + 0.008*\"market\" + 0.008*\"startup\" + '\n",
      "  '0.008*\"want\" + 0.008*\"customer\"'),\n",
      " (14,\n",
      "  '0.013*\"cell\" + 0.011*\"human\" + 0.010*\"lock\" + 0.009*\"thread\" + 0.006*\"many\" '\n",
      "  '+ 0.006*\"neuron\" + 0.006*\"study\" + 0.005*\"type\" + 0.005*\"antibody\" + '\n",
      "  '0.005*\"know\"'),\n",
      " (2,\n",
      "  '0.012*\"make\" + 0.009*\"get\" + 0.007*\"take\" + 0.007*\"time\" + 0.006*\"even\" + '\n",
      "  '0.006*\"people\" + 0.005*\"work\" + 0.005*\"come\" + 0.005*\"know\" + 0.005*\"go\"'),\n",
      " (29,\n",
      "  '0.028*\"headline\" + 0.014*\"read\" + 0.011*\"write\" + 0.010*\"book\" + '\n",
      "  '0.008*\"get\" + 0.007*\"make\" + 0.006*\"rocksdb\" + 0.006*\"reader\" + '\n",
      "  '0.005*\"title\" + 0.004*\"article\"'),\n",
      " (13,\n",
      "  '0.014*\"sound\" + 0.012*\"assistant\" + 0.008*\"phone\" + 0.008*\"notification\" + '\n",
      "  '0.007*\"regex\" + 0.007*\"apple\" + 0.006*\"tag\" + 0.005*\"make\" + '\n",
      "  '0.005*\"cohesion\" + 0.005*\"many\"'),\n",
      " (23,\n",
      "  '0.015*\"work\" + 0.011*\"people\" + 0.009*\"make\" + 0.007*\"need\" + 0.006*\"new\" + '\n",
      "  '0.006*\"get\" + 0.006*\"problem\" + 0.006*\"time\" + 0.006*\"team\" + 0.005*\"take\"'),\n",
      " (21,\n",
      "  '0.019*\"word\" + 0.013*\"create\" + 0.011*\"use\" + 0.009*\"type\" + 0.009*\"text\" + '\n",
      "  '0.008*\"get\" + 0.007*\"make\" + 0.007*\"character\" + 0.006*\"figure\" + '\n",
      "  '0.006*\"also\"'),\n",
      " (12,\n",
      "  '0.014*\"study\" + 0.013*\"brain\" + 0.009*\"people\" + 0.008*\"experience\" + '\n",
      "  '0.007*\"human\" + 0.007*\"social\" + 0.006*\"symptom\" + 0.005*\"also\" + '\n",
      "  '0.005*\"child\" + 0.005*\"research\"'),\n",
      " (16,\n",
      "  '0.044*\"card\" + 0.009*\"cosine_similarity\" + 0.008*\"qldb\" + 0.007*\"immutable\" '\n",
      "  '+ 0.006*\"contour\" + 0.005*\"immutability\" + 0.005*\"probability\" + '\n",
      "  '0.005*\"sort\" + 0.005*\"pipx\" + 0.005*\"blockchain\"'),\n",
      " (1,\n",
      "  '0.020*\"design\" + 0.014*\"model\" + 0.012*\"user\" + 0.010*\"product\" + '\n",
      "  '0.009*\"system\" + 0.009*\"use\" + 0.008*\"new\" + 0.008*\"make\" + 0.006*\"learn\" + '\n",
      "  '0.006*\"process\"'),\n",
      " (5,\n",
      "  '0.056*\"datum\" + 0.011*\"use\" + 0.007*\"science\" + 0.007*\"make\" + 0.006*\"data\" '\n",
      "  '+ 0.006*\"machine\" + 0.006*\"big\" + 0.006*\"human\" + 0.005*\"system\" + '\n",
      "  '0.005*\"information\"'),\n",
      " (6,\n",
      "  '0.017*\"spark\" + 0.017*\"framework\" + 0.015*\"hadoop\" + 0.015*\"datum\" + '\n",
      "  '0.013*\"use\" + 0.008*\"process\" + 0.008*\"react\" + 0.008*\"job\" + '\n",
      "  '0.007*\"memory\" + 0.006*\"task\"'),\n",
      " (9,\n",
      "  '0.012*\"search\" + 0.009*\"make\" + 0.007*\"help\" + 0.007*\"question\" + '\n",
      "  '0.006*\"get\" + 0.006*\"time\" + 0.006*\"also\" + 0.006*\"people\" + 0.005*\"well\" + '\n",
      "  '0.005*\"ask\"'),\n",
      " (7,\n",
      "  '0.047*\"datum\" + 0.014*\"use\" + 0.011*\"model\" + 0.011*\"value\" + '\n",
      "  '0.010*\"dataset\" + 0.009*\"feature\" + 0.008*\"column\" + 0.008*\"plot\" + '\n",
      "  '0.008*\"number\" + 0.007*\"see\"'),\n",
      " (27,\n",
      "  '0.014*\"case\" + 0.014*\"virus\" + 0.013*\"people\" + 0.011*\"health\" + '\n",
      "  '0.009*\"patient\" + 0.008*\"coronavirus\" + 0.007*\"death\" + 0.007*\"say\" + '\n",
      "  '0.007*\"study\" + 0.007*\"country\"')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3954825088041496\n",
      "\n",
      "\n",
      " ------------ \n",
      "\n",
      "\n",
      "[(7,\n",
      "  '0.038*\"datum\" + 0.016*\"value\" + 0.014*\"use\" + 0.012*\"plot\" + '\n",
      "  '0.010*\"dataset\" + 0.010*\"number\" + 0.009*\"feature\" + 0.009*\"point\" + '\n",
      "  '0.009*\"column\" + 0.008*\"see\"'),\n",
      " (26,\n",
      "  '0.024*\"service\" + 0.013*\"use\" + 0.011*\"application\" + 0.011*\"datum\" + '\n",
      "  '0.010*\"run\" + 0.008*\"kubernete\" + 0.008*\"create\" + 0.008*\"log\" + '\n",
      "  '0.007*\"need\" + 0.007*\"provide\"'),\n",
      " (32,\n",
      "  '0.009*\"make\" + 0.008*\"people\" + 0.008*\"get\" + 0.007*\"know\" + 0.007*\"feel\" + '\n",
      "  '0.007*\"think\" + 0.007*\"see\" + 0.006*\"take\" + 0.006*\"life\" + 0.006*\"even\"'),\n",
      " (40,\n",
      "  '0.016*\"feel\" + 0.016*\"get\" + 0.012*\"work\" + 0.012*\"day\" + 0.010*\"make\" + '\n",
      "  '0.010*\"time\" + 0.010*\"people\" + 0.008*\"know\" + 0.007*\"thing\" + '\n",
      "  '0.007*\"sleep\"'),\n",
      " (15,\n",
      "  '0.017*\"dog\" + 0.016*\"get\" + 0.009*\"make\" + 0.009*\"people\" + 0.008*\"start\" + '\n",
      "  '0.008*\"path\" + 0.007*\"video\" + 0.007*\"play\" + 0.006*\"conda\" + 0.006*\"see\"'),\n",
      " (24,\n",
      "  '0.020*\"dough\" + 0.016*\"end\" + 0.015*\"pizza\" + 0.009*\"chef\" + '\n",
      "  '0.008*\"project\" + 0.007*\"cold_shower\" + 0.007*\"intern\" + 0.007*\"enum\" + '\n",
      "  '0.007*\"animation\" + 0.007*\"straighten\"'),\n",
      " (27,\n",
      "  '0.011*\"lift\" + 0.009*\"caffeine\" + 0.008*\"admission\" + 0.008*\"high\" + '\n",
      "  '0.008*\"new\" + 0.008*\"blood_pressure\" + 0.007*\"trial\" + 0.006*\"imposter\" + '\n",
      "  '0.006*\"tea\" + 0.006*\"resort\"'),\n",
      " (43,\n",
      "  '0.049*\"song\" + 0.032*\"music\" + 0.026*\"tree\" + 0.020*\"node\" + 0.016*\"track\" '\n",
      "  '+ 0.012*\"album\" + 0.011*\"recursive\" + 0.009*\"lyric\" + 0.007*\"first\" + '\n",
      "  '0.007*\"artist\"'),\n",
      " (23,\n",
      "  '0.013*\"work\" + 0.012*\"people\" + 0.008*\"human\" + 0.008*\"make\" + 0.007*\"job\" '\n",
      "  '+ 0.006*\"also\" + 0.006*\"problem\" + 0.006*\"new\" + 0.005*\"get\" + '\n",
      "  '0.005*\"research\"'),\n",
      " (12,\n",
      "  '0.021*\"study\" + 0.013*\"brain\" + 0.010*\"child\" + 0.010*\"people\" + '\n",
      "  '0.009*\"disorder\" + 0.009*\"antibiotic\" + 0.007*\"research\" + 0.007*\"risk\" + '\n",
      "  '0.006*\"diagnosis\" + 0.006*\"also\"'),\n",
      " (5,\n",
      "  '0.061*\"datum\" + 0.029*\"design\" + 0.017*\"user\" + 0.012*\"use\" + 0.010*\"make\" '\n",
      "  '+ 0.009*\"tool\" + 0.006*\"data\" + 0.006*\"science\" + 0.006*\"create\" + '\n",
      "  '0.006*\"big\"'),\n",
      " (31,\n",
      "  '0.019*\"medium\" + 0.018*\"article\" + 0.018*\"story\" + 0.017*\"write\" + '\n",
      "  '0.014*\"content\" + 0.014*\"writer\" + 0.013*\"post\" + 0.013*\"make\" + '\n",
      "  '0.012*\"get\" + 0.011*\"read\"'),\n",
      " (46,\n",
      "  '0.020*\"code\" + 0.018*\"create\" + 0.018*\"use\" + 0.012*\"run\" + 0.009*\"make\" + '\n",
      "  '0.009*\"need\" + 0.008*\"file\" + 0.008*\"project\" + 0.008*\"add\" + 0.008*\"also\"'),\n",
      " (6,\n",
      "  '0.029*\"pipeline\" + 0.025*\"feature\" + 0.013*\"datum\" + 0.011*\"use\" + '\n",
      "  '0.010*\"project\" + 0.009*\"store\" + 0.009*\"sql_query\" + 0.008*\"model\" + '\n",
      "  '0.007*\"estimation\" + 0.006*\"engine\"'),\n",
      " (41,\n",
      "  '0.010*\"electron\" + 0.007*\"translate\" + 0.006*\"also\" + 0.006*\"translation\" + '\n",
      "  '0.006*\"take\" + 0.005*\"see\" + 0.005*\"text\" + 0.005*\"go\" + 0.005*\"climate\" + '\n",
      "  '0.004*\"solution\"'),\n",
      " (38,\n",
      "  '0.023*\"time\" + 0.019*\"work\" + 0.019*\"get\" + 0.016*\"make\" + 0.012*\"thing\" + '\n",
      "  '0.012*\"take\" + 0.011*\"need\" + 0.009*\"learn\" + 0.009*\"start\" + 0.009*\"want\"'),\n",
      " (20,\n",
      "  '0.035*\"vaccine\" + 0.010*\"people\" + 0.009*\"get\" + 0.006*\"also\" + '\n",
      "  '0.006*\"reaction\" + 0.006*\"sentiment\" + 0.006*\"make\" + 0.005*\"look\" + '\n",
      "  '0.005*\"datum\" + 0.005*\"methane\"'),\n",
      " (34,\n",
      "  '0.017*\"istio\" + 0.011*\"virtual\" + 0.010*\"device\" + 0.009*\"traffic\" + '\n",
      "  '0.009*\"proxy\" + 0.008*\"route\" + 0.007*\"atom\" + 0.007*\"cluster\" + '\n",
      "  '0.006*\"unsupervised\" + 0.006*\"use\"'),\n",
      " (9,\n",
      "  '0.039*\"search\" + 0.007*\"make\" + 0.007*\"datum\" + 0.007*\"get\" + 0.006*\"site\" '\n",
      "  '+ 0.006*\"website\" + 0.006*\"trade\" + 0.006*\"information\" + 0.005*\"also\" + '\n",
      "  '0.005*\"time\"'),\n",
      " (13,\n",
      "  '0.011*\"webpack\" + 0.010*\"spacy\" + 0.007*\"spiritually\" + 0.007*\"applause\" + '\n",
      "  '0.006*\"cloudformation\" + 0.005*\"psychopath\" + 0.005*\"find\" + 0.005*\"life\" + '\n",
      "  '0.005*\"see\" + 0.005*\"much\"')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in (10,20,30,50):\n",
    "    # number of topics\n",
    "    num_topics = i\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           per_word_topics=True)\n",
    "    # Print the Keyword in the 10 topics\n",
    "    pprint(lda_model.print_topics())\n",
    "    doc_lda = lda_model[corpus]\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_lda)\n",
    "    print(\"\\n\\n ------------ \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4207cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f394c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e3588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c629325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d7e2339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a10d6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b01be8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = int(len(corpus))\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
    "               corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dbd795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/540 [05:40<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Stop argument for islice() must be None or an integer: 0 <= x <= sys.maxsize.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\models\\ldamulticore.py:224\u001b[0m, in \u001b[0;36mLdaMulticore.update\u001b[1;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     lencorpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m alpha:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# iterare through beta values\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m beta:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;66;03m# get the coherence score for the given parameters\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m         cv \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_coherence_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m# Save the model results\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         model_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation_Set\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(corpus_title[i])\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mcompute_coherence_values\u001b[1;34m(corpus, dictionary, k, a, b)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_coherence_values\u001b[39m(corpus, dictionary, k, a, b):\n\u001b[1;32m----> 4\u001b[0m     lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLdaMulticore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     coherence_model_lda \u001b[38;5;241m=\u001b[39m CoherenceModel(model\u001b[38;5;241m=\u001b[39mlda_model, texts\u001b[38;5;241m=\u001b[39mdata_lemmatized, dictionary\u001b[38;5;241m=\u001b[39mid2word, coherence\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m coherence_model_lda\u001b[38;5;241m.\u001b[39mget_coherence()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\models\\ldamulticore.py:186\u001b[0m, in \u001b[0;36mLdaMulticore.__init__\u001b[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(alpha, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m alpha \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto-tuning alpha not implemented in LdaMulticore; use plain LdaModel.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLdaMulticore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminimum_probability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminimum_probability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimum_phi_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminimum_phi_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_word_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_word_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\models\\ldamodel.py:520\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    518\u001b[0m use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    519\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    523\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\models\\ldamulticore.py:227\u001b[0m, in \u001b[0;36mLdaMulticore.update\u001b[1;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput corpus stream has no len(); counting documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 227\u001b[0m     lencorpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m corpus)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lencorpus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    229\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLdaMulticore.update() called with an empty corpus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\gensim\\utils.py:1084\u001b[0m, in \u001b[0;36mClippedCorpus.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Stop argument for islice() must be None or an integer: 0 <= x <= sys.maxsize."
     ]
    }
   ],
   "source": [
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c508519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b589242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDAvis_data_filepath = os.getcwd()+\n",
    "\n",
    "LDAvis_data_filepath = os.path.join(f'{os.getcwd()}/results/ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'{os.getcwd()}/results/ldavis_prepared_'+ str(num_topics) +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01700263",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac85123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(LDAvis_prepared, f'ldavis_prepared_{str(num_topics)}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1330ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
